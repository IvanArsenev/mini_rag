# ğŸ¤– RAG Telegram Bot

A Telegram bot that helps you search across your own uploaded documents using semantic embeddings and LLM-based answers.
Powered by Elasticsearch, LangChain Ollama, and aiogram 3. ğŸ”ğŸ§ 

---

## âœ¨ Features

* ğŸ“‚ Upload `.txt` files directly to Telegram
* ğŸ§  Automatically splits and indexes text into Elasticsearch
* ğŸ” Supports semantic and keyword search
* ğŸ’¬ Uses LLM (LLaMA-3) to answer questions based on your documents
* ğŸ§¼ Manage your data â€” delete uploaded files anytime
* âš™ï¸ Asynchronous and scalable architecture via aiogram 3 + asyncio

---

## ğŸ“Œ Prepare to start

1. **Clone the repository**:

   ```shell
   git clone https://github.com/IvanArsenev/mini_rag.git
   cd smart-search-tg-bot
   ```

2. Create `config.py` file:

   ```python
   TOKEN = '' # Your Telegram bot token
   ELASTIC_HOST = 'http://elasticsearch:9200'
   OLLAMA_HOST = 'http://ollama:11434'
   ```

3. (Optional) Install Ollama locally if not using Docker:
   ğŸ‘‰ [https://ollama.ai/download](https://ollama.ai/download)

   Then pull the model:

   ```shell
   ollama pull llama3
   ```

---

## ğŸ³ Docker Compose Setup (Recommended)

This project includes a `docker-compose.yml` for one-command deployment of:

* ğŸ§  **Ollama** (LLaMA-3 model)
* ğŸ—ƒï¸ **Elasticsearch** (document storage)
* ğŸ¤– **Telegram Bot**

### 1ï¸âƒ£ Create `.env` file:

```env
BOT_TOKEN=your_telegram_bot_token_here
```

### 2ï¸âƒ£ Example `docker-compose.yml`:

```yaml
version: "3.9"

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.1
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - es_data:/usr/share/elasticsearch/data

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped

  bot:
    build: .
    container_name: smart-search-bot
    env_file: .env
    depends_on:
      - elasticsearch
      - ollama
    volumes:
      - ./tmp:/app/tmp
    restart: unless-stopped
    command: ["python", "bot.py"]

volumes:
  es_data:
  ollama_models:
```

### 3ï¸âƒ£ Build and run all services:

```shell
docker-compose up -d --build
```

### 4ï¸âƒ£ Pull the LLaMA model inside the Ollama container (only once):

```shell
docker exec -it ollama ollama pull llama3
```

Your bot will now connect to both **Elasticsearch** and **Ollama** automatically. ğŸš€

---

## ğŸ› ï¸ Manual Setup (Without Docker)

1. **Create and activate virtual environment** (tested on Python 3.11+):

   ```shell
   python -m venv .venv
   .\.venv\Scripts\activate     # Windows
   source .venv/bin/activate    # Linux/macOS
   ```

2. **Install dependencies:**

   ```shell
   pip install -r requirements.txt
   ```

3. **Run Elasticsearch and Ollama locally:**

   ```shell
   docker run -d -p 9200:9200 -e "discovery.type=single-node" elasticsearch:8.15.1
   ollama serve
   ollama pull llama3
   ```

4. **Start the bot:**

   ```shell
   python bot.py
   ```

---

## âš™ï¸ How It Works

* `bot.py` â€” handles user interactions, uploads, and LLM queries
* `elastic.py` â€” manages Elasticsearch integration and vector search
* `models.py` â€” FSM states for user interaction
* `config.py` â€” stores tokens and hosts

**Pipeline overview:**

1. User uploads a `.txt` file
2. File â†’ chunks (100 words each)
3. Each chunk â†’ embedding via **OllamaEmbeddings**
4. Stored in **Elasticsearch**
5. Search = semantic + keyword results â†’ passed to **LLaMA-3** for final answer

---

## ğŸ’¬ Example Interaction

**User:**

> /start
> Upload file

**Bot:**
âœ… â€œFile successfully indexed!â€

**User:**

> What does the document mention about reinforcement learning?

**Bot:**
ğŸ§  â€œIt discusses reward optimization methods.â€
ğŸ“„ *(Relevant excerpts attached)*

---

## ğŸ“š Project Structure

```
smart-search-tg-bot/
â”‚
â”œâ”€â”€ bot.py              # Telegram bot logic
â”œâ”€â”€ elastic.py          # Elasticsearch + embedding integration
â”œâ”€â”€ models.py           # FSM states
â”œâ”€â”€ config.py           # Config for tokens and hosts
â”œâ”€â”€ tmp/                # Temp folder for uploads
â”œâ”€â”€ requirements.txt    # Dependencies
â””â”€â”€ docker-compose.yml  # One-command deployment
```

---

## ğŸ§© Tech Stack

* **Python 3.11+**
* **aiogram 3**
* **Elasticsearch 8**
* **LangChain Ollama (LLaMA-3)**
* **AsyncIO / Aiofiles**
* **Docker & Compose**

---

## ğŸ§  Notes

* Make sure **Ollama** and **Elasticsearch** are reachable by hostname from the bot container
* Uploaded files must be `.txt` â‰¤ 5 MB
* Each Telegram user has their own isolated Elasticsearch index
* Fully offline â€” no external APIs required
* Works great for research notes, documentation, and private data

---

## ğŸ§° Example `requirements.txt`

```
aiogram==3.13.1
aiofiles==24.1.0
elasticsearch==8.15.1
langchain_community==0.3.9
langchain_ollama==0.2.2
chardet==5.2.0
```